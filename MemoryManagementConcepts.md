## Address Binding and MMU

When we run a program, the logical addresses in your code need to be converted to physical memory locations. This conversion happens through address binding.

Address binding can happen at three different times:
1. **Compile time**: When the compiler knows exactly where the program will reside in memory. If the location changes, recompilation is necessary.
2. **Load time**: When the loader modifies the program's addresses if the starting location changes.
3. **Execution time**: When binding is delayed until run time. This requires special hardware support (the MMU).

The **Memory Management Unit (MMU)** is the hardware device that translates logical addresses to physical addresses at runtime. It works by adding a relocation register value to every address generated by the CPU.

For example, if a program starts at physical address 14000:
- Program instruction refers to location 346
- MMU adds 14000
- Physical address accessed is 14346

This dynamic relocation through the MMU enables important capabilities like virtual memory.

## Dynamic Loading and Linking

**Dynamic Loading** means loading routines into memory only when they're needed, not in advance. This saves memory space, especially for rarely used code like error handling routines.

**Dynamic Linking** is similar but involves linking to shared libraries at runtime rather than embedding all code in the executable. The advantages are:
- Smaller executable file sizes
- Shared code among multiple programs (one copy in memory)
- Easier updates (change the library without recompiling programs)

The Windows DLL (Dynamic Link Library) system is a classic example of dynamic linking.

## Swapping and Contiguous Allocation

**Swapping** temporarily moves entire processes from memory to a backing store (usually disk) and back again. This allows more processes to run than would fit in memory at once.

**Contiguous allocation** gives each process a single continuous section of memory. There are two main approaches:
- **Fixed partitioning**: Memory is divided into fixed-sized partitions
- **Variable partitioning**: Partitions are created dynamically based on process sizes

This leads us to the problem of fragmentation.

## Fragmentation

**External fragmentation** occurs when there's enough total memory for a process, but it's not contiguous. Imagine having 200MB free memory in total, but only in 50MB fragments, when a process needs 100MB.

**Internal fragmentation** happens when memory is allocated in fixed blocks, and a process gets more memory than it needs. If a process needs 90KB but gets a 100KB partition, 10KB is wasted.

**Compaction** can solve external fragmentation by moving processes in memory to create larger free blocks, but it's computationally expensive.

## Paging

**Paging** is an elegant solution that divides physical memory into fixed-sized blocks called frames and divides logical memory into blocks of the same size called pages. When a process runs, its pages are loaded into available frames.

With paging:
- The physical address space can be non-contiguous
- External fragmentation is eliminated
- Internal fragmentation is limited to only the last page of a process

Each logical address consists of:
- A page number (p)
- An offset (d)

The **page table** maps logical pages to physical frames. To access memory:
1. CPU generates logical address
2. Page number is used as index into page table
3. Corresponding frame number is retrieved
4. Frame number is combined with offset to form physical address

For example, with 4KB pages and a 32-bit address space:
- Upper 20 bits form the page number
- Lower 12 bits form the offset

## Translation Lookaside Buffer (TLB) and Effective Access Time

Paging requires two memory accesses: one for the page table and one for the data. To speed this up, a special fast-lookup hardware cache called the **Translation Lookaside Buffer (TLB)** is used.

The TLB contains entries mapping page numbers to frame numbers. When the CPU generates a logical address:
1. Page number is sent to TLB
2. If found (TLB hit), frame number is immediately retrieved
3. If not found (TLB miss), system must access the page table in memory

**Effective Access Time (EAT)** measures the average time for a memory access with paging and TLB:

EAT = (Hit Ratio × TLB Access Time) + (1 - Hit Ratio) × (TLB Access Time + Memory Access Time for Page Table + Memory Access Time for Data)

For example, if:
- Memory access time = 100 ns
- TLB access time = 20 ns
- TLB hit ratio = 0.8

Then: EAT = 0.8 × (20 + 100) + 0.2 × (20 + 100 + 100) = 0.8 × 120 + 0.2 × 220 = 96 + 44 = 140 ns

## Valid/Invalid Bits and Shared Pages

**Valid/Invalid Bits** in page tables indicate whether a page is in the process's logical address space. This allows:
- Protection from accessing memory outside a process's space
- Implementing sparse address spaces where not all logical addresses are used

**Shared Pages** allow multiple processes to share the same physical memory for certain pages, which is useful for:
- Shared code (e.g., common libraries)
- Shared data structures
- Reducing memory use and disk transfers

## Segmentation

While paging divides memory by size, **segmentation** divides it by function. A program is a collection of segments (like code, stack, heap, data). Each segment has a name and length.

Logical addresses consist of:
- A segment number (s)
- An offset (d)

The **segment table** maps segment numbers to physical base addresses and contains the segment's length for bounds checking.

For example, if the stack segment (segment 0) starts at location 4000 and you access offset 53:
- Logical address: (0, 53)
- Physical address: 4000 + 53 = 4053

Segmentation can cause external fragmentation but aligns better with how programmers think about memory.

## Virtual Memory

**Virtual memory** creates the illusion of having more memory than physically exists by using disk as an extension of RAM. It allows:
- Running programs larger than physical memory
- Simplifying programming by abstracting physical memory constraints
- Increasing the degree of multiprogramming

## Demand Paging

**Demand paging** brings pages into memory only when needed, rather than loading the entire process at once. This reduces:
- I/O needed to load a program
- Memory usage
- Response time for users
- Degree of multiprogramming possible

To implement demand paging, the page table includes:
- **Valid/invalid bit**: "1" means the page is in memory, "0" means not in memory
- **Present/absent bit**: Similar purpose with different terminology
- Additional information like a "dirty bit" to track modifications

## Memory Protection

**Memory protection** prevents processes from accessing each other's memory. This is implemented through:
- **Base and limit registers**: Define the memory range a process can access
- **Valid/invalid bits**: Prevent access to pages not in the process's address space
- **Protection bits**: Define read/write/execute permissions for pages

## Page Faults and Handling

A **page fault** occurs when a program accesses a page that's not currently in physical memory. Here's what happens:

1. CPU checks the page table and sees the page isn't in memory
2. The operating system is invoked (trap)
3. OS checks if the reference is valid (in the process's address space)
4. If invalid, terminate the process; if valid, continue
5. Find a free frame (or make one free by replacing a page)
6. Read the needed page from disk into the free frame
7. Update the page table
8. Restart the instruction that caused the fault

The **page fault rate** significantly impacts performance. If p is the probability of a page fault (like 0.001, or one fault per 1000 memory accesses), and it takes 10ms to service a fault, the effective access time becomes:
EAT = (1 - p) × memory access time + p × page fault time

## Page Replacement and Algorithms

When memory is full and a page fault occurs, the system must choose a page to remove (**victim page**) to make room. This is where **page replacement algorithms** come in.

**First-In-First-Out (FIFO)** replaces the oldest page in memory. It's simple but often performs poorly because the oldest page might still be in frequent use.

**Optimal page replacement** replaces the page that will not be used for the longest time in the future. It has the lowest possible page fault rate but requires knowledge of future references, making it impractical but useful as a theoretical benchmark.

**Least Recently Used (LRU)** replaces the page that hasn't been used for the longest time. It works on the principle of locality of reference (recently used pages are likely to be used again). LRU performs well but is expensive to implement precisely.

For example, with a sequence of page references: 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1 and 3 frames:

- FIFO might generate 15 page faults
- Optimal might generate 9 page faults
- LRU might generate 12 page faults

There are also approximation algorithms like:
- **LRU Clock Algorithm**: Uses a "use bit" and a circular list to approximate LRU
- **Counting-based algorithms**: Track how many times each page has been referenced

## Thrashing

**Thrashing** occurs when a system spends more time paging than executing useful work. It happens when the working set (the set of pages a process actively uses) doesn't fit in available memory.

Signs of thrashing include:
- High CPU utilization with low throughput
- Excessive page faults
- Low system performance

To prevent thrashing:
- Ensure each process has enough frames for its working set
- Use local replacement policies rather than global ones
- Adjust the degree of multiprogramming (reduce active processes)
